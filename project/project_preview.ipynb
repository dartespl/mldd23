{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFeaturizer(Featurizer): #uznajmy ze mamy taki featurizer\n",
    "    def __call__(self, df):\n",
    "        graphs = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            \n",
    "            edges = []\n",
    "            for bond in mol.GetBonds():\n",
    "                begin = bond.GetBeginAtomIdx()\n",
    "                end = bond.GetEndAtomIdx()\n",
    "                edges.append((begin, end))  # TODO: Add edges in both directions\n",
    "            edges = np.array(edges)\n",
    "            \n",
    "            nodes = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                results = one_of_k_encoding_unk(atom.GetAtomicNum(), range(11)) + one_of_k_encoding(\n",
    "                    atom.GetDegree(), range(11)\n",
    "                ) + one_of_k_encoding_unk(\n",
    "                    atom.GetImplicitValence(), range(11)\n",
    "                ) + [atom.GetIsAromatic()] + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(11)\n",
    "                ) + [atom.GetNumImplicitHs(), atom.GetFormalCharge(), atom.GetNumRadicalElectrons(), atom.GetIsAromatic()]\n",
    "                nodes.append(results)\n",
    "            nodes = np.array(nodes)\n",
    "            \n",
    "            graphs.append((nodes, edges.T))\n",
    "            labels.append(y)\n",
    "        labels = np.array(labels)\n",
    "        return [Data(\n",
    "            x=torch.FloatTensor(x), \n",
    "            edge_index=torch.LongTensor(edge_index), \n",
    "            y=torch.FloatTensor([y])\n",
    "        ) for ((x, edge_index), y) in zip(graphs, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warstwa attention pooling\n",
    "class MyAttentionModule(torch.nn.Module): # zakladamy ze atom ma 49 featerow\n",
    "    def __init__(self, groupFeatures=1):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(49, 49) # dla zebrania informacji od sasiadow\n",
    "        self.gates = { # do wyliczenia atencji dla kazdej grupy cech - jest ich 9\n",
    "            'AtomicNum': torch.nn.Linear(11, 1),\n",
    "            'Degree': torch.nn.Linear(11, 1),\n",
    "            'ImplicitValence': torch.nn.Linear(11, 1),\n",
    "            'IsAromatic': torch.nn.Linear(1, 1),\n",
    "            'TotalNumHs': torch.nn.Linear(11, 1),\n",
    "            'NumImplicitHs': torch.nn.Linear(1, 1),\n",
    "            'FormalCharge': torch.nn.Linear(1, 1),\n",
    "            'NumRadicalElectrons': torch.nn.Linear(1, 1),\n",
    "            'IsAromatic': torch.nn.Linear(1, 1)\n",
    "        }\n",
    "        \n",
    "        self.feats = { # do transformacji grupy cech w wektor, na razie dziala tylko dla groupFeatures=1\n",
    "            'AtomicNum': torch.nn.Linear(11, groupFeatures),\n",
    "            'Degree': torch.nn.Linear(11, groupFeatures),\n",
    "            'ImplicitValence': torch.nn.Linear(11, groupFeatures),\n",
    "            'IsAromatic': torch.nn.Linear(1, groupFeatures),\n",
    "            'TotalNumHs': torch.nn.Linear(11, groupFeatures),\n",
    "            'NumImplicitHs': torch.nn.Linear(1, groupFeatures),\n",
    "            'FormalCharge': torch.nn.Linear(1, groupFeatures),\n",
    "            'NumRadicalElectrons': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsAromatic': torch.nn.Linear(1, groupFeatures)\n",
    "        }\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv(x, edge_index)\n",
    "        # print(x.shape)\n",
    "        subgroups = []\n",
    "        subgroups.append(self.gates['AtomicNum'](x[:,0:11]))\n",
    "        subgroups.append(self.gates['Degree'](x[:,11:22]))\n",
    "        subgroups.append(self.gates['ImplicitValence'](x[:,22:33]))\n",
    "        subgroups.append(self.gates['IsAromatic'](x[:,33:34]))\n",
    "        subgroups.append(self.gates['TotalNumHs'](x[:,34:45]))\n",
    "        subgroups.append(self.gates['NumImplicitHs'](x[:,45:46]))\n",
    "        subgroups.append(self.gates['FormalCharge'](x[:,46:47]))\n",
    "        subgroups.append(self.gates['NumRadicalElectrons'](x[:,47:48]))\n",
    "        subgroups.append(self.gates['IsAromatic'](x[:,48:49]))\n",
    "        logits = torch.cat(subgroups, dim=-1) # dla np x o ksztalcie (1200, 49) bedziemy mieli tensor istotnosci (1200, 9)\n",
    "        logits = gap(logits, batch=batch) # nie pisal Pan o tym, ale chyba chcemy miec wektor atencji dla kazdej czasteczki a nie dla kazdego atomu z osobna, wiec usredniam dla czasteczki\n",
    "        # czyli dla batch_size=64 bedziemy mieli pooling: (1200, 9) -> (64, 9)\n",
    "        attention = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        subgroups = []\n",
    "        subgroups.append(self.feats['AtomicNum'](x[:,0:11]))\n",
    "        subgroups.append(self.feats['Degree'](x[:,11:22]))\n",
    "        subgroups.append(self.feats['ImplicitValence'](x[:,22:33]))\n",
    "        subgroups.append(self.feats['IsAromatic'](x[:,33:34]))\n",
    "        subgroups.append(self.feats['TotalNumHs'](x[:,34:45]))\n",
    "        subgroups.append(self.feats['NumImplicitHs'](x[:,45:46]))\n",
    "        subgroups.append(self.feats['FormalCharge'](x[:,46:47]))\n",
    "        subgroups.append(self.feats['NumRadicalElectrons'](x[:,47:48]))\n",
    "        subgroups.append(self.feats['IsAromatic'](x[:,48:49]))\n",
    "        x = torch.cat(subgroups, dim=-1) # kazda grupe przerzucamy przez warstwe liniowa i konkatenujemy: (1200, 49) -> (1200, 9)\n",
    "        \n",
    "        for i, atom_features in enumerate(x):\n",
    "            x[i] *= attention[batch[i]] #przemnazamy grupy featerow przez ich istotnosci\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przykladowe jej uzycie w sieci grafowej\n",
    "class GraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, n_features=49, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.myAttentionModule = MyAttentionModule(1)\n",
    "        self.conv1 = GCNConv(9, hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, int(hidden_size))\n",
    "        self.conv3 = GCNConv(int(hidden_size), int(hidden_size))\n",
    "        self.linear = torch.nn.Linear(int(hidden_size), 1)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x, att = self.myAttentionModule(x, edge_index, batch) #nie rozumiem czemu mialbym robic pooling, skoro nasz modul uzywamy na poczatku forward - jeszcze przed warstwami konwolucyjnymi\n",
    "        #a chcemy moc pozniej uzyc nasza wyuczona warstwe do roznych innych datasetow i architektor, jak dobrze rozumiem\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        x = gap(x, batch)\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = self.linear(x)\n",
    "\n",
    "        return out, att"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
