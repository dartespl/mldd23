{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "100%|██████████| 853k/853k [00:01<00:00, 475kiB/s] \n",
      "Loading...\n",
      "Done!\n",
      "[01:47:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:43] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tdc.single_pred.adme import ADME\n",
    "from tdc import Evaluator\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self, y_column, smiles_col='Drug', **kwargs):\n",
    "        self.y_column = y_column\n",
    "        self.smiles_col = smiles_col\n",
    "        self.__dict__.update(kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ECFPFeaturizer(Featurizer):\n",
    "    def __init__(self, y_column, radius=2, length=1024, **kwargs):\n",
    "        self.radius = radius\n",
    "        self.length = length\n",
    "        super().__init__(y_column, **kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        fingerprints = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, self.radius, nBits=self.length)\n",
    "            fingerprints.append(fp)\n",
    "            labels.append(y)\n",
    "        fingerprints = np.array(fingerprints)\n",
    "        labels = np.array(labels)\n",
    "        return fingerprints, labels\n",
    "\n",
    "data = ADME('Solubility_AqSolDB')\n",
    "split = data.get_split()\n",
    "rmse = Evaluator(name = 'RMSE')\n",
    "\n",
    "featurizer = ECFPFeaturizer(y_column='Y')\n",
    "X_train, y_train = featurizer(split['train'])\n",
    "X_valid, y_valid = featurizer(split['valid'])\n",
    "X_test, y_test = featurizer(split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise ValueError(\"input {0} not in allowable set{1}:\".format(\n",
    "            x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "class GraphFeaturizer(Featurizer):\n",
    "    def __call__(self, df):\n",
    "        graphs = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            \n",
    "            edges = []\n",
    "            for bond in mol.GetBonds():\n",
    "                begin = bond.GetBeginAtomIdx()\n",
    "                end = bond.GetEndAtomIdx()\n",
    "                edges.append((begin, end))  # TODO: Add edges in both directions\n",
    "            edges = np.array(edges)\n",
    "            \n",
    "            nodes = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                # print(atom.GetAtomicNum(), atom.GetNumImplicitHs(), atom.GetTotalNumHs(), atom.GetSymbol(), atom.GetNumExplicitHs(), atom.GetTotalValence())\n",
    "                results = one_of_k_encoding_unk(atom.GetAtomicNum(), range(11)) + one_of_k_encoding(\n",
    "                    atom.GetDegree(), range(11)\n",
    "                ) + one_of_k_encoding_unk(\n",
    "                    atom.GetImplicitValence(), range(11)\n",
    "                ) + [atom.GetIsAromatic()] + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(11)\n",
    "                ) + [atom.GetNumImplicitHs(), atom.GetFormalCharge(), atom.GetNumRadicalElectrons(), atom.GetIsAromatic()] # TODO: Add atom features as a list, you can use one_of_k_encodings defined above\n",
    "                # print(results)\n",
    "                nodes.append(results)\n",
    "            nodes = np.array(nodes)\n",
    "            \n",
    "            graphs.append((nodes, edges.T))\n",
    "            labels.append(y)\n",
    "        labels = np.array(labels)\n",
    "        return [Data(\n",
    "            x=torch.FloatTensor(x), \n",
    "            edge_index=torch.LongTensor(edge_index), \n",
    "            y=torch.FloatTensor([y])\n",
    "        ) for ((x, edge_index), y) in zip(graphs, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = GraphFeaturizer('Y')\n",
    "graph = featurizer(split['test'].iloc[:1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:47:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[01:47:56] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "batch_size = 64\n",
    "train_loader = GraphDataLoader(featurizer(split['train']), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = GraphDataLoader(featurizer(split['valid']), batch_size=batch_size)\n",
    "test_loader = GraphDataLoader(featurizer(split['test']), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.ones((10, 2))\n",
    "t2 = torch.zeros((10, 3))\n",
    "\n",
    "torch.softmax(t1, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool as gap\n",
    "\n",
    "class MyAttentionModule(torch.nn.Module): # zakladamy ze atom ma 49 featerow\n",
    "    def __init__(self, groupFeatures=1):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(49, 49) # dla zebrania informacji od sasiadow\n",
    "        self.gates = { # do wyliczenia atencji dla kazdej grupy cech - jest ich 9\n",
    "            'AtomicNum': torch.nn.Linear(11, 1),\n",
    "            'Degree': torch.nn.Linear(11, 1),\n",
    "            'ImplicitValence': torch.nn.Linear(11, 1),\n",
    "            'IsAromatic': torch.nn.Linear(1, 1),\n",
    "            'TotalNumHs': torch.nn.Linear(11, 1),\n",
    "            'NumImplicitHs': torch.nn.Linear(1, 1),\n",
    "            'FormalCharge': torch.nn.Linear(1, 1),\n",
    "            'NumRadicalElectrons': torch.nn.Linear(1, 1),\n",
    "            'IsAromatic': torch.nn.Linear(1, 1)\n",
    "        }\n",
    "        \n",
    "        self.feats = { # do transformacji grupy cech w wektor, na razie dziala tylko dla groupFeatures=1\n",
    "            'AtomicNum': torch.nn.Linear(11, groupFeatures),\n",
    "            'Degree': torch.nn.Linear(11, groupFeatures),\n",
    "            'ImplicitValence': torch.nn.Linear(11, groupFeatures),\n",
    "            'IsAromatic': torch.nn.Linear(1, groupFeatures),\n",
    "            'TotalNumHs': torch.nn.Linear(11, groupFeatures),\n",
    "            'NumImplicitHs': torch.nn.Linear(1, groupFeatures),\n",
    "            'FormalCharge': torch.nn.Linear(1, groupFeatures),\n",
    "            'NumRadicalElectrons': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsAromatic': torch.nn.Linear(1, groupFeatures)\n",
    "        }\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv(x, edge_index)\n",
    "        # print(x.shape)\n",
    "        subgroups = []\n",
    "        subgroups.append(self.gates['AtomicNum'](x[:,0:11]))\n",
    "        subgroups.append(self.gates['Degree'](x[:,11:22]))\n",
    "        subgroups.append(self.gates['ImplicitValence'](x[:,22:33]))\n",
    "        subgroups.append(self.gates['IsAromatic'](x[:,33:34]))\n",
    "        subgroups.append(self.gates['TotalNumHs'](x[:,34:45]))\n",
    "        subgroups.append(self.gates['NumImplicitHs'](x[:,45:46]))\n",
    "        subgroups.append(self.gates['FormalCharge'](x[:,46:47]))\n",
    "        subgroups.append(self.gates['NumRadicalElectrons'](x[:,47:48]))\n",
    "        subgroups.append(self.gates['IsAromatic'](x[:,48:49]))\n",
    "        logits = torch.cat(subgroups, dim=-1) # dla np x o ksztalcie (1200, 49) bedziemy mieli tensor istotnosci (1200, 9)\n",
    "        logits = gap(logits, batch=batch) # nie pisal Pan o tym, ale chyba chcemy miec wektor atencji dla kazdej czasteczki a nie dla kazdego atomu z osobna, wiec usredniam dla czasteczki\n",
    "        # czyli dla batch_size=64 bedziemy mieli pooling: (1200, 9) -> (64, 9)\n",
    "        attention = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        subgroups = []\n",
    "        subgroups.append(self.feats['AtomicNum'](x[:,0:11]))\n",
    "        subgroups.append(self.feats['Degree'](x[:,11:22]))\n",
    "        subgroups.append(self.feats['ImplicitValence'](x[:,22:33]))\n",
    "        subgroups.append(self.feats['IsAromatic'](x[:,33:34]))\n",
    "        subgroups.append(self.feats['TotalNumHs'](x[:,34:45]))\n",
    "        subgroups.append(self.feats['NumImplicitHs'](x[:,45:46]))\n",
    "        subgroups.append(self.feats['FormalCharge'](x[:,46:47]))\n",
    "        subgroups.append(self.feats['NumRadicalElectrons'](x[:,47:48]))\n",
    "        subgroups.append(self.feats['IsAromatic'](x[:,48:49]))\n",
    "        x = torch.cat(subgroups, dim=-1) # kazda grupe przerzucamy przez warstwe liniowa i konkatenujemy: (1200, 49) -> (1200, 9)\n",
    "        \n",
    "        for i, atom_features in enumerate(x):\n",
    "            x[i] *= attention[batch[i]] #przemnazamy grupy featerow przez ich istotnosci\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(torch.nn.Module):  # TODO: assign hyperparameters to attributes and define the forward pass\n",
    "    def __init__(self, hidden_size, n_features=49, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.myAttentionModule = MyAttentionModule(1)\n",
    "        self.conv1 = GCNConv(27, hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, int(hidden_size))\n",
    "        self.conv3 = GCNConv(int(hidden_size), int(hidden_size))\n",
    "        self.linear = torch.nn.Linear(int(hidden_size), 1)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x, att = self.myAttentionModule(x, edge_index, batch) #nie rozumiem czemu mialbym robic pooling, skoro nasz modul uzywamy na poczatku forward - jeszcze przed warstwami konwolucyjnymi\n",
    "        #a chcemy moc pozniej uzyc nasza wyuczona warstwe do roznych innych datasetow i architektor, jak dobrze rozumiem\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        x = gap(x, batch)\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = self.linear(x)\n",
    "\n",
    "        return out, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf9a706e5ba452aa1a9cd35d6e45b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94f30c1fb4a418fb48cc92bc09c4b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (27) must match the size of tensor b (9) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 40\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m preds\n\u001b[0;32m     39\u001b[0m \u001b[39m# training\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m model \u001b[39m=\u001b[39m train(train_loader, valid_loader)\n\u001b[0;32m     42\u001b[0m \u001b[39m# evaluation\u001b[39;00m\n\u001b[0;32m     43\u001b[0m predictions \u001b[39m=\u001b[39m predict(model, test_loader)\n",
      "Cell \u001b[1;32mIn[67], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, valid_loader)\u001b[0m\n\u001b[0;32m     16\u001b[0m x, edge_index, batch, y \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mbatch, data\u001b[39m.\u001b[39my\n\u001b[0;32m     17\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m preds, att \u001b[39m=\u001b[39m model(x, edge_index, batch)\n\u001b[0;32m     19\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(preds, y\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\rufus\\.conda\\envs\\mldd23\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[66], line 12\u001b[0m, in \u001b[0;36mGraphNeuralNetwork.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, batch):\n\u001b[1;32m---> 12\u001b[0m     x, att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmyAttentionModule(x, edge_index, batch)\n\u001b[0;32m     13\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x, edge_index)\n\u001b[0;32m     14\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrelu()\n",
      "File \u001b[1;32mc:\\Users\\rufus\\.conda\\envs\\mldd23\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[65], line 72\u001b[0m, in \u001b[0;36mMyAttentionModule.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m i, atom_features \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x):\n\u001b[0;32m     71\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupFeatures):\n\u001b[1;32m---> 72\u001b[0m         x[i\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupFeatures \u001b[39m+\u001b[39m j] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m attention[batch[i]]\n\u001b[0;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m x, attention\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (27) must match the size of tensor b (9) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "def train(train_loader, valid_loader):\n",
    "    # hyperparameters definition\n",
    "    hidden_size = 512\n",
    "    epochs = 20\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # model preparation\n",
    "    model = GraphNeuralNetwork(hidden_size)  # TODO: you can add more hyperparameters if needed\n",
    "    model.train()\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate) # TODO: define an optimizer\n",
    "    loss_fn = torch.nn.MSELoss()  # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        for data in tqdm(train_loader, leave=False):\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    # evaluation loop\n",
    "    preds_batches = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            \n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            preds_batches.append(preds.cpu().detach().numpy())\n",
    "            print(att)\n",
    "    preds = np.concatenate(preds_batches)\n",
    "    return preds\n",
    "\n",
    "\n",
    "# training\n",
    "model = train(train_loader, valid_loader)\n",
    "\n",
    "# evaluation\n",
    "predictions = predict(model, test_loader)\n",
    "\n",
    "rmse_score = rmse(y_test, predictions.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 1.64\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE = {rmse_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldd23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
